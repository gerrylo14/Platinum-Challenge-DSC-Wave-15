{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca9773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gerrylorinanto/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Mar/2024 10:55:46] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Mar/2024 10:55:48] \"GET /docs/ HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Mar/2024 10:55:48] \"\u001b[36mGET /flasgger_static/swagger-ui-bundle.js HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Mar/2024 10:55:48] \"\u001b[36mGET /flasgger_static/swagger-ui.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Mar/2024 10:55:48] \"\u001b[36mGET /flasgger_static/swagger-ui-standalone-preset.js HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Mar/2024 10:55:48] \"\u001b[36mGET /flasgger_static/lib/jquery.min.js HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Mar/2024 10:55:48] \"GET /docs.json HTTP/1.1\" 200 -\n",
      "ERROR:__main__:Exception on /text_sentiment_countvectorizer [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gerrylorinanto/anaconda3/lib/python3.11/site-packages/flask/app.py\", line 1463, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gerrylorinanto/anaconda3/lib/python3.11/site-packages/flask/app.py\", line 872, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gerrylorinanto/anaconda3/lib/python3.11/site-packages/flask/app.py\", line 870, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gerrylorinanto/anaconda3/lib/python3.11/site-packages/flask/app.py\", line 855, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/r6/qx9w1zwx0zjgzg0qdmn1pwph0000gn/T/ipykernel_2305/3148332465.py\", line 181, in text_sentiment_countvectorizer\n",
      "    feature = count_vectorizer.transform(text)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gerrylorinanto/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 1425, in transform\n",
      "    self._check_vocabulary()\n",
      "  File \"/Users/gerrylorinanto/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 508, in _check_vocabulary\n",
      "    raise NotFittedError(\"Vocabulary not fitted or provided\")\n",
      "sklearn.exceptions.NotFittedError: Vocabulary not fitted or provided\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Mar/2024 10:55:58] \"\u001b[35m\u001b[1mPOST /text_sentiment_countvectorizer HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import sqlite3\n",
    "import nltk\n",
    "\n",
    "from flask import Flask, jsonify\n",
    "app = Flask(__name__)\n",
    "\n",
    "from flask import request\n",
    "from flasgger import Swagger, LazyJSONEncoder, swag_from\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "app.json_encoder = LazyJSONEncoder\n",
    "\n",
    "swagger_template = {\n",
    "    \"info\": {\n",
    "        \"title\": \"API Documentation Platinum Challenge\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"description\": \"Dokumentasi API untuk Prediksi Sentimen\",\n",
    "    },\n",
    "    \"host\": \"127.0.0.1:5000\",\n",
    "}\n",
    "swagger_config = {\n",
    "    \"headers\": [],\n",
    "    \"specs\": [\n",
    "        {\"endpoint\": \"docs\", \"route\": \"/docs.json\",}\n",
    "    ],\n",
    "    \"static_url_path\": \"/flasgger_static\",\n",
    "    \"swagger_ui\": True,\n",
    "    \"specs_route\": \"/docs/\",\n",
    "}\n",
    "swagger = Swagger(app, template=swagger_template, config=swagger_config)\n",
    "\n",
    "# Inisialisasi Database\n",
    "def initialize_database():\n",
    "    conn = sqlite3.connect('goldchallenge_database.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Buat tabel jika belum ada\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS cleaned_data (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            original_text TEXT,\n",
    "            cleaned_text TEXT\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Panggil fungsi inisialisasi pada saat aplikasi dijalankan\n",
    "initialize_database()\n",
    "\n",
    "# Load kamus kata alay dari kamusalay.csv\n",
    "df_stopword = pd.read_csv('/Users/gerrylorinanto/Gelo/Gerry/Binar/Platinum Challenge/stopwords_indonesian.csv', encoding = 'latin1')\n",
    "df_alay = pd.read_csv('/Users/gerrylorinanto/Gelo/Gerry/Binar/Platinum Challenge/new_kamusalay.csv',  encoding = 'latin1')\n",
    "\n",
    "#cleansing\n",
    "def preprocess_text(text):\n",
    "    #Menghapus USER,RT,URL\n",
    "    text = re.sub(r'USER|\\bRT\\b|URL',' ',text)\n",
    "    \n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Menghapus /n\n",
    "    text =  re.sub(r'\\\\n',' ',text)\n",
    "    \n",
    "    # Hapus emotikon dan karakter khusus\n",
    "    text = re.sub(r'[^\\w\\d\\s]', '', text)\n",
    "\n",
    "    # mengganti spasi yang berlebihan\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Menghapus kata dan huruf yang bergabung\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    # Mengganti kata yang berulang\n",
    "    text = re.sub(r'\\b(\\w+)\\1\\b', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "#removing stopwords\n",
    "stopwords = df_stopword['Stopword'].tolist()\n",
    "def remove_stopwords(text):\n",
    "    list_stopwords = text.split()\n",
    "    return ' '.join([text for text in list_stopwords if text not in stopwords])\n",
    "\n",
    "#normalization\n",
    "kamus_alay = dict(zip(df_alay['anakjakartaasikasik'], df_alay['anak jakarta asyik asyik']))\n",
    "def normalize(text):\n",
    "    for word in kamus_alay:\n",
    "        return ' '.join([kamus_alay[word] if word in kamus_alay else word for word in text.split(' ')])\n",
    "\n",
    "def cleansing_text(text):\n",
    "    text = preprocess_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = normalize(text)\n",
    "    return text\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    \n",
    "    def stem_text(tokens):\n",
    "        return stemmer.stem(tokens)\n",
    "\n",
    "    text = text.apply(stem_text)\n",
    "    return text\n",
    "\n",
    "max_features = 100000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', lower=True)\n",
    "sentiment = ['negative', 'neutral', 'positive']\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "file = open('/Users/gerrylorinanto/Gelo/Gerry/Binar/Platinum Challenge/Resource LSTM/x_pad_sequences.pickle','rb')\n",
    "feature_file_from_lstm = pickle.load(file)\n",
    "\n",
    "model_file_from_lstm = load_model('/Users/gerrylorinanto/Gelo/Gerry/Binar/Platinum Challenge/Model LSTM/model_lstm.h5')\n",
    "\n",
    "file = open('//Users/gerrylorinanto/Gelo/Gerry/Binar/Platinum Challenge/Resource MLP/feature.p','rb')\n",
    "feature_file_from_MLP = pickle.load(file)\n",
    "\n",
    "#model_file_from_MLP = load_model('/Users/gerrylorinanto/Gelo/Gerry/Binar/Platinum Challenge/Model MLP/model_MLP.pickle')\n",
    "with open('/Users/gerrylorinanto/Gelo/Gerry/Binar/Platinum Challenge/Model MLP/model_MLP.pickle', 'rb') as file:\n",
    "    model_file_from_MLP = pickle.load(file)\n",
    "\n",
    "@swag_from(\n",
    "    \"/Users/gerrylorinanto/Gelo/Gerry/Binar/Platinum Challenge/Docs YML/team.yml\", methods=['GET'],\n",
    ")\n",
    "@app.route('/', methods=['GET'])\n",
    "def hello_world():\n",
    "    json_response = {\n",
    "        \"Kelompok 2 DSC Wave 15\",\n",
    "        \"PLATINUM - CHALLENGE\"\n",
    "    }\n",
    "    response_data = jsonify({\"kelompok\": list(json_response)})  # Mengonversi set menjadi list\n",
    "    return response_data\n",
    "\n",
    "@swag_from(\n",
    "    '/Users/gerrylorinanto/Gelo/Gerry/Binar/Platinum Challenge/Docs YML/text_LSTM.yml', methods=['POST'],\n",
    ")\n",
    "@app.route('/text_sentiment_LSTM', methods=['POST'])\n",
    "def text_sentiment_LSTM():\n",
    "    original_text = request.form.get('text')\n",
    "    text = [cleansing_text(original_text)]  # Fix the function name\n",
    "    feature = tokenizer.texts_to_sequences(text)\n",
    "    feature = pad_sequences(feature, maxlen=feature_file_from_lstm.shape[1])\n",
    "    prediction = model_file_from_lstm.predict(feature)\n",
    "    get_sentiment = sentiment[np.argmax(prediction[0])]\n",
    "\n",
    "    json_response = {\n",
    "        \"status_code\": 200,\n",
    "        \"description\": \"Teks yang akan diproses\",\n",
    "        \"data\": {\n",
    "            'text': original_text,\n",
    "            'sentiment': get_sentiment\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "@swag_from(\n",
    "    '/Users/gerrylorinanto/Gelo/Gerry/Binar/Platinum Challenge/Docs YML/text_MLP.yml', methods=['POST'],\n",
    ")\n",
    "@app.route('/text_sentiment_countvectorizer', methods=['POST'])\n",
    "def text_sentiment_countvectorizer():\n",
    "    original_text = request.form.get('text')\n",
    "    text = [cleansing_text(original_text)]\n",
    "\n",
    "    # Menggunakan CountVectorizer untuk ekstraksi fitur\n",
    "    feature = count_vectorizer.transform(text)\n",
    "\n",
    "    # Memprediksi sentimen menggunakan model neural network\n",
    "    predictions = model_file_from_MLP.predict(feature)\n",
    "    get_sentiment = sentiment[np.argmax(predictions[0])]\n",
    "\n",
    "    json_response = {\n",
    "        \"status_code\": 200,\n",
    "        \"description\": \"Teks yang akan diproses menggunakan CountVectorizer\",\n",
    "        \"data\": {\n",
    "            'text': original_text,\n",
    "            'sentiment': get_sentiment\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "\n",
    "@swag_from(\n",
    "    '/Users/gerrylorinanto/Gelo/Gerry/Binar/Platinum Challenge/Docs YML/file_LSTM.yml', methods=['POST'],\n",
    ")\n",
    "@app.route('/File_Sentiment_LSTM', methods=['POST'])\n",
    "def File_Sentiment_LSTM(): \n",
    "    file = request.files.getlist(\"file\")[0]\n",
    "    df = pd.read_csv(file, encoding=\"ISO-8859-1\")\n",
    "    \n",
    "    texts = df['text'].apply(cleansing_text)\n",
    "    features = tokenizer.texts_to_sequences(texts)\n",
    "    features = pad_sequences(features, maxlen=feature_file_from_lstm.shape[1])\n",
    "    predictions = model_file_from_lstm.predict(features)\n",
    "    get_sentiments = [sentiment[np.argmax(pred)] for pred in predictions]\n",
    "\n",
    "    json_response = {\n",
    "        \"status_code\": 200,\n",
    "        \"description\": \"Data yang akan diproses dari file CSV\",\n",
    "        \"data\": {\n",
    "            'texts': list(texts),\n",
    "            'sentiments': get_sentiments\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f721b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
